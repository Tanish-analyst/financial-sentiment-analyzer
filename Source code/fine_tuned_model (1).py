# -*- coding: utf-8 -*-
"""fine_tuned_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19L93v0nXcbZEbqQ1HkJbxrf9PvCeczS9
"""

import pandas as pd

# Load the Excel file
df = pd.read_excel("/content/chatgpt senti.xlsx")
df

label_mapping = {"neutral": 0, "positive": 1, "negative": 2}
df['sentiment'] = df['sentiment'].map(label_mapping)
df

from datasets import Dataset
dataset = Dataset.from_pandas(df)
dataset=dataset.shuffle(seed=42)

dataset

dataset = dataset.rename_column("sentiment", "label")

split_dataset = dataset.train_test_split(test_size=0.2)

# Access the train and test datasets
train_dataset = split_dataset['train']
test_dataset = split_dataset['test']

train_dataset,test_dataset

from transformers import AutoTokenizer

model_name = "yiyanghkust/finbert-tone"  # Or any other model
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(example):
    return tokenizer(example["headlines"], truncation=True, padding="max_length")

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_test = test_dataset.map(tokenize_function, batched=True)

tokenized_train

tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask", "token_type_ids", "label"])
tokenized_test.set_format(type="torch", columns=["input_ids", "attention_mask", "token_type_ids", "label"])

from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

!huggingface-cli login

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True
)

# Commented out IPython magic to ensure Python compatibility.
# %pip install evaluate
import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    return accuracy.compute(predictions=predictions, references=labels)

from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.push_to_hub()

